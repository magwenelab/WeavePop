# =================================================================================================
#   Check minimal version
# =================================================================================================

from snakemake.utils import min_version
import sys
import os

min_version("8.2.1")

# =================================================================================================
#   Setup config file
# =================================================================================================

if not config:
    configfile: "config/config.yaml"

# =================================================================================================
#   Define output and logs directories
# =================================================================================================

if config["project_directory"]:
    os.makedirs(config["project_directory"], exist_ok=True)
    if config["run_id"]:
        OUTPUT = Path(config["project_directory"], "results_" + config["run_id"])
        LOGS = Path(config["project_directory"], "logs_" + config["run_id"])
    else:
        OUTPUT = Path(config["project_directory"], "results")
        LOGS = Path(config["project_directory"], "logs")
else:
    if config["run_id"]:
        OUTPUT = Path("results_" + config["run_id"])
        LOGS = Path("logs_" + config["run_id"])
    else:
        OUTPUT = Path("results")
        LOGS = Path("logs")

# =================================================================================================
#   Save stdout and stderr to log file
# =================================================================================================

os.makedirs(LOGS, exist_ok=True)
log_file_path = LOGS / "weavepop.log"

if os.path.exists(log_file_path):
    base, ext = os.path.splitext(log_file_path)
    counter = 1
    new_log_file_path = f"{base}_{counter}{ext}"
    while os.path.exists(new_log_file_path):
        counter += 1
        new_log_file_path = f"{base}_{counter}{ext}"
    log_file_path = new_log_file_path

sys.stdout = open(log_file_path, "a")
sys.stderr = sys.stdout

# =================================================================================================
#   Define names of directories
# =================================================================================================

SAMPLES_DIR_NAME = "01.Samples"
DATASET_DIR_NAME = "02.Dataset"
REFS_DIR_NAME = "03.References"
INTDIR_NAME = "04.Intermediate_files"
TEMPDIR_NAME = "04.Temporary_files"

# =================================================================================================
#   Load rules
# =================================================================================================

# ------------------Analysis workflow--------------------------------------------------------------
if config["workflow"] == "analysis":

    include: "rules/common.smk"
    include: "rules/snippy.smk"
    include: "rules/quality_filter.smk"
    include: "rules/database.smk"
    include: "rules/ref_processing.smk"

    # ------------------Reference annotation rules-------------------------------------------------
    if config["annotate_references"]["activate"]:

        include: "rules/references_annotate.smk"

    if not config["annotate_references"]["activate"]:

        include: "rules/references_no_annotate.smk"

    # ------------------Module rules---------------------------------------------------------------
    if config["database"]["activate"]:

        include: "rules/annotation.smk"
        include: "rules/cnv.smk"
        include: "rules/snpeff.smk"
        include: "rules/depth_quality_features.smk"
        if config["cnv"]["repeats"]["use_container"]:
            include: "rules/repeatmasker_containers.smk"
        else:
            include: "rules/repeatmasker.smk"

    else:
        if config["annotation"]["activate"] or config["depth_quality_features"]["activate"] or config["cnv"]["activate"]:

            include: "rules/annotation.smk"
        
        if config["depth_quality_features"]["activate"] or config["cnv"]["activate"]:
            
            include: "rules/depth_quality_features.smk"
        
        if config["snpeff"]["activate"]:

            include: "rules/snpeff.smk"

        if config["cnv"]["activate"]: 

            include: "rules/cnv.smk"

            if config["cnv"]["repeats"]["use_container"]:
                include: "rules/repeatmasker_containers.smk"
            else:
                include: "rules/repeatmasker.smk"

    if config["plotting"]["activate"]:

        include: "rules/plots.smk"
        include: "rules/plots_dataset.smk"


# ------------------Join datasets workflow---------------------------------------------------------
elif config["workflow"] == "join_datasets":

    include: "rules/common_join_datasets.smk"
    include: "rules/join_datasets.smk"

# ------------------No workflow selected message----------------------------------------------------
else:
    print("Workflow must be one of 'analysis' or 'join_datasets'.", flush=True)
    print("Check the spelling in the config/config.yaml file.", flush=True)
    print("Exiting...", flush=True)
    exit(1)

# =================================================================================================
#   On success messages
# =================================================================================================

onsuccess:
    print(r"""
    __        __                   ____             
    \ \      / /__  __ ___   _____|  _ \ ___  _ __  
     \ \ /\ / / _ \/ _` \ \ / / _ \ |_) / _ \| '_ \ 
      \ V  V /  __/ (_| |\ V /  __/  __/ (_) | |_) |
       \_/\_/ \___|\__,_| \_/ \___|_|   \___/| .__/ 
                                             |_|    
    """, flush=True)
    if config["workflow"] == "analysis":
        print("The analysis workflow finished successfully!", flush=True)
        original_metadata = METADATA_UNFILTERED
        filtered_metadata = pd.read_csv(rules.quality_filter.output.metadata, header=0)
        if original_metadata["sample"].nunique() == filtered_metadata["sample"].nunique():
            print("No samples were removed.", flush=True)
        else:
            print(
                f"The quality filter removed "
                f"{original_metadata['sample'].nunique()- filtered_metadata['sample'].nunique()} "
                f"samples. See 02.Dataset/depth_quality/"
                f"mapping_stats.tsv to check the quality warning of the removed samples.",
                flush=True,
            )
    elif config["workflow"] == "join_datasets":
        print("The join datasets workflow finished successfully!", flush=True)
    print("                                   ", flush=True)
    print("Enjoy science!", flush=True)

# =================================================================================================
#   On error messages
# =================================================================================================

onerror:
    print(r"""
    __        __                   ____             
    \ \      / /__  __ ___   _____|  _ \ ___  _ __  
     \ \ /\ / / _ \/ _` \ \ / / _ \ |_) / _ \| '_ \ 
      \ V  V /  __/ (_| |\ V /  __/  __/ (_) | |_) |
       \_/\_/ \___|\__,_| \_/ \___|_|   \___/| .__/ 
                                             |_|    
    """, flush=True)
    print("An error occurred during the workflow execution.", flush=True)
    print("Above, find the rule that failed and check its log file for error details.", flush=True)
    print("                                   ", flush=True)
    print("Exiting...", flush=True)
    exit(1)

# =================================================================================================
#   Define final output
# =================================================================================================

if config["workflow"] == "analysis":

    rule all:
        input:
            get_unfiltered_output(),
            get_filtered_output(),
            get_dataset_output(),

elif config["workflow"] == "join_datasets":

    rule all:
        input:
            get_final_output(),
