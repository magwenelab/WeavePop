# =================================================================================================
#   Check minimal version
# =================================================================================================

from snakemake.utils import min_version
import sys

min_version("8.2.1")

# =================================================================================================
#   Setup config file
# =================================================================================================

if not config:
    configfile: "config/config.yaml"

# =================================================================================================
#   Define names of directories
# =================================================================================================

SAMPLES_DIR_NAME = "01.Samples"
DATASET_DIR_NAME = "02.Dataset"
REFS_DIR_NAME = "03.References"
INTDIR_NAME = "04.Intermediate_files"
TEMPDIR_NAME = "04.Temporary_files"

# =================================================================================================
#   Load rules
# =================================================================================================

# ------------------Analysis workflow--------------------------------------------------------------
if config["workflow"] == "analysis":

    include: "rules/common.smk"
    include: "rules/snippy.smk"
    include: "rules/quality_filter.smk"
    include: "rules/annotation.smk"
    include: "rules/database.smk"

    # ------------------Reference annotation rules-------------------------------------------------
    if config["annotate_references"]["activate"]:

        include: "rules/references_annotate.smk"

    if not config["annotate_references"]["activate"]:

        include: "rules/references_no_annotate.smk"


    # ------------------Module rules---------------------------------------------------------------
    if config["database"]["activate"]:

        include: "rules/cnv.smk"
        include: "rules/snpeff.smk"
        include: "rules/depth_quality_features.smk"
        if config["cnv"]["repeats"]["use_container"]:
            include: "rules/repeatmasker_containers.smk"
        else:
            include: "rules/repeatmasker.smk"


    else:

        if config["cnv"]["activate"] or config["plotting"]["activate"]:

            include: "rules/cnv.smk"
            if config["cnv"]["repeats"]["use_container"]:
                include: "rules/repeatmasker_containers.smk"
            else:
                include: "rules/repeatmasker.smk"

        if config["depth_quality_features"]["activate"] or config["plotting"]["activate"]:

            include: "rules/depth_quality_features.smk"

        if config["snpeff"]["activate"]:

            include: "rules/snpeff.smk"
    
    if config["plotting"]["activate"]:

        include: "rules/plots.smk"
        include: "rules/plots_dataset.smk"


# ------------------Join datasets workflow---------------------------------------------------------
elif config["workflow"] == "join_datasets":

    include: "rules/common_join_datasets.smk"
    include: "rules/join_datasets.smk"

# ------------------No workflow selected message----------------------------------------------------
else:
    print("Workflow must be one of 'analysis' or 'join_datasets'.", flush=True)
    print("Check the spelling in the config/config.yaml file.", flush=True)
    print("Exiting...", flush=True)
    exit(1)

# =================================================================================================
#   On start checks
# =================================================================================================

# onstart:

#     print("Executed command:", " ".join(sys.argv))
#     print("                                   ", flush=True)
#     profile_config_path = None
#     for arg in sys.argv: # Check if --profile option is used
#         if arg.startswith("--profile"):     
#             profile_path = arg.split("=")[1] if "=" in arg else sys.argv[sys.argv.index(arg) + 1]
#             profile_config_path = os.path.join(profile_path, "config.yaml")
#             break
#     if profile_config_path and os.path.exists(profile_config_path):
#         print("Command line options from profile config file:", flush=True)
#         with open(profile_config_path, 'r') as file:
#             for line in file:
#                 if line.strip() and not line.strip().startswith("#"):
#                     print(line.strip(), flush=True)
#         print("                                   ", flush=True)

#     params_config_path = None
#     for arg in sys.argv: # Check if --configfile option is used
#         if arg.startswith("--configfile"):     
#             params_config_path = arg.split("=")[1] if "=" in arg else sys.argv[sys.argv.index(arg) + 1]
#             break
#         else:
#             params_config_path = "config/config.yaml"
#     if params_config_path and os.path.exists(params_config_path):
#         print("Parameters from config file:", flush=True)
#         with open(params_config_path, 'r') as file:
#             for line in file:
#                 if line.strip() and not line.strip().startswith("#"):
#                     print(line.strip(), flush=True)
#         print("                                   ", flush=True)

# =================================================================================================
#   On success messages
# =================================================================================================

onsuccess:
    print("                                   ", flush=True)
    print(" _           _   _      _   _   _  ", flush=True)
    print("|_ | | |\\ | |_  |_| |  |_| | | |_|", flush=True)
    print("|  |_| | \\| |_| | | |_ |   |_| |  ", flush=True)
    print("                                   ", flush=True)
    print("                                   ", flush=True)
    if config["workflow"] == "analysis":
        print("The analysis workflow finished successfully!", flush=True)
        original_metadata = UNFILT_SAMPLE_TABLE
        filtered_metadata = pd.read_csv(rules.quality_filter.output.metadata, header=0)
        if original_metadata["sample"].nunique() == filtered_metadata["sample"].nunique():
            print("All samples passed the quality filter.", flush=True)
        else:
            print(
                f"The quality filter removed "
                f"{original_metadata['sample'].nunique()- filtered_metadata['sample'].nunique()} "
                f"samples. See 04.Intermediate_files/02.Dataset/depth_quality/"
                f"unfiltered_mapping_stats.tsv to check the quality warning of the removed samples.",
                flush=True,
            )
    elif config["workflow"] == "join_datasets":
        print("The join datasets workflow finished successfully!", flush=True)
    print("                                   ", flush=True)
    print("Enjoy science!", flush=True)

# =================================================================================================
#   On error messages
# =================================================================================================

onerror:
    print("                                   ", flush=True)
    print(" _           _   _      _   _   _  ", flush=True)
    print("|_ | | |\\ | |_  |_| |  |_| | | |_|", flush=True)
    print("|  |_| | \\| |_| | | |_ |   |_| |  ", flush=True)
    print("                                   ", flush=True)
    print("                                   ", flush=True)
    print("An error occurred during the workflow execution.", flush=True)
    print("Check the log file for more information.", flush=True)
    print("                                   ", flush=True)
    print("Exiting...", flush=True)
    exit(1)

# =================================================================================================
#   Define final output
# =================================================================================================

if config["workflow"] == "analysis":

    rule all:
        input:
            get_unfiltered_output(),
            get_filtered_output(),
            get_dataset_output(),

elif config["workflow"] == "join_datasets":

    rule all:
        input:
            get_final_output(),
